{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CÃ¢u 1"
      ],
      "metadata": {
        "id": "RszXC6h65swZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYb_0Rrx6uqH",
        "outputId": "36fee222-6cc3-4aa8-81f1-f27c3004cd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  000     : 5003.65269695053\n",
            "  000 (01): 4657.172584623218\n",
            "  000 (02): 3365.034302387373\n",
            "  001     : 1000.1699160323785\n",
            "  002     : 370.599125971233\n",
            "  003     : 170.79274319244163\n",
            "  004     : 139.04740889383112\n",
            "  005     : 132.62466479393703\n",
            "  006     : 125.05796091397508\n",
            "  007     : 112.53704561539874\n",
            "  008     : 108.18153378401932\n",
            "  009     : 104.45210965695046\n",
            "  010     : 102.08766717737652\n",
            "  011     : 98.05256070773319\n",
            "  012     : 96.36938095294425\n",
            "  013     : 94.61611441602909\n",
            "  014     : 93.53571285522473\n",
            "  015     : 92.6664481613885\n",
            "  016     : 92.10040159836699\n",
            "  017     : 91.87929668644608\n",
            "  018     : 91.83195309204196\n",
            "  019     : 91.74244213135154\n",
            "  020     : 91.68765465002288\n",
            "  021     : 91.66398642431064\n",
            "  022     : 91.65094552007164\n",
            "  023     : 91.6443074393712\n",
            "  024     : 91.64127548454759\n",
            "  025     : 91.63934960336218\n",
            "  026     : 91.63875210350817\n",
            "  027     : 91.63827931863901\n",
            "  028     : 91.63807136951287\n",
            "  029     : 91.63798262880883\n",
            "  030     : 91.63795159358793\n",
            "  031     : 91.63794017738991\n",
            "  032     : 91.63792681719491\n",
            "  033     : 91.63793844847436\n",
            "  033 (01): 91.63792483182583\n",
            "  034     : 91.63792307476392\n",
            "  035     : 91.63792180618657\n",
            "  036     : 91.63792126931234\n",
            "  037     : 91.63792109780005\n",
            "Accuracy: 0.7695076152722721\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from math import log\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from itertools import chain\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "BOS_TOKEN, BOS_IDX = '<bos>', 0\n",
        "\n",
        "\n",
        "def read_corpus(filename):\n",
        "    with open(filename) as file:\n",
        "        lines = [line.strip().split() for line in file]\n",
        "\n",
        "    data, X, Y = [], [], []\n",
        "    for words in lines:\n",
        "        if not words:\n",
        "            if X: data.append((X, Y))\n",
        "            X, Y = [], []\n",
        "        else:\n",
        "            X.append(words[:-1])\n",
        "            Y.append(words[-1])\n",
        "\n",
        "    if X: data.append((X, Y))\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_text_features_at_position(text_tokens, position):\n",
        "    \"\"\"\n",
        "    Extracts text features at a specified position in a list of text tokens.\n",
        "\n",
        "    Args:\n",
        "    text_tokens (list): A list of text tokens, where each token is represented as a list containing\n",
        "                        the token itself and its part-of-speech tag.\n",
        "    position (int): The position in the list to extract features for.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of extracted features, including unigrams and bigrams based on the specified position.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    For text_tokens:\n",
        "\n",
        "    [['Confidence', 'NN'], ['in', 'IN'], ['the', 'DT'], ['pound', 'NN'], ['is', 'VBZ'], ['widely', 'RB'],\n",
        "     ['expected', 'VBN'], ['to', 'TO'], ['take', 'VB'], ['another', 'DT'], ['sharp', 'JJ'], ['dive', 'NN'],\n",
        "     ['if', 'IN'], ['trade', 'NN'], ['figures', 'NNS'], ['for', 'IN'], ['September', 'NNP'], [',', ','],\n",
        "     ['due', 'JJ'], ['for', 'IN'], ['release', 'NN'], ['tomorrow', 'NN'], [',', ','], ['fail', 'VB'],\n",
        "     ['to', 'TO'], ['show', 'VB'], ['a', 'DT'], ['substantial', 'JJ'], ['improvement', 'NN'], ['from', 'IN'],\n",
        "     ['July', 'NNP'], ['and', 'CC'], ['August', 'NNP'], [\"'s\", 'POS'], ['near-record', 'JJ'], ['deficits', 'NNS'],\n",
        "     ['.', '.']]\n",
        "\n",
        "      When position = 0:\n",
        "    features = ['unigram[0]:Confidence', 'unigram[+1]:in', 'bigram[0]:Confidence in', 'unigram[+2]:the']\n",
        "\n",
        "    When position = 5:\n",
        "    features = ['unigram[0]:widely', 'unigram[+1]:expected', 'bigram[0]:widely expected', 'unigram[+2]:to', 'unigram[-1]:is', 'bigram[-1]:is widely', 'unigram[-2]:pound']\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    length = len(text_tokens)\n",
        "\n",
        "    features = list()\n",
        "    features.append('unigram[0]:' + text_tokens[position][0])\n",
        "\n",
        "    if position < length-1:\n",
        "        features.append('unigram[+1]:' + (text_tokens[position+1][0]))\n",
        "        features.append('bigram[0]:{} {}'.format(text_tokens[position][0], text_tokens[position+1][0]))\n",
        "\n",
        "        if position < length-2:\n",
        "            features.append('unigram[+2]:' + (text_tokens[position+2][0]))\n",
        "\n",
        "    if position > 0:\n",
        "        features.append('unigram[-1]:' + (text_tokens[position-1][0]))\n",
        "        features.append('bigram[-1]:{} {}'.format(text_tokens[position-1][0], text_tokens[position][0]))\n",
        "\n",
        "        if position > 1:\n",
        "            features.append('unigram[-2]:' + (text_tokens[position-2][0]))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "class FeatureSet():\n",
        "    feature_dict, observation_set, empirical_counts = dict(), set(), Counter()\n",
        "    num_features, label_dict, label_array = 0, {BOS_TOKEN: BOS_IDX}, [BOS_TOKEN]\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def process_corpus(self, data):\n",
        "        for X, Y in data:\n",
        "            prev_y = BOS_IDX\n",
        "            for t, char in enumerate(X):\n",
        "                y = self.label_dict.get(Y[t], len(self.label_dict))\n",
        "                if Y[t] not in self.label_dict:\n",
        "                    self.label_dict[Y[t]] = y\n",
        "                    self.label_array.append(Y[t])\n",
        "                self._add(prev_y, y, X, t)\n",
        "                prev_y = y\n",
        "\n",
        "    def load(self, feature_dict, num_features, label_array):\n",
        "        self.num_features = num_features\n",
        "        self.label_array = label_array\n",
        "        self.label_dict = {label: i for label, i in enumerate(label_array)}\n",
        "        self.feature_dict = self.deserialize_feature_dict(feature_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_features\n",
        "\n",
        "    def _add(self, prev_y, y, X, t):\n",
        "        for feature_string in extract_text_features_at_position(X, t):\n",
        "            #print(\"=-=-=: \",feature_string)\n",
        "            if feature_string not in self.feature_dict:\n",
        "                self.feature_dict[feature_string] = {}\n",
        "\n",
        "            for pair in [(prev_y, y), (-1, y)]:\n",
        "                if pair not in self.feature_dict[feature_string]:\n",
        "                    self.feature_dict[feature_string][pair] = self.num_features\n",
        "                    self.num_features += 1\n",
        "\n",
        "                feature_id = self.feature_dict[feature_string][pair]\n",
        "                self.empirical_counts[feature_id] += 1\n",
        "\n",
        "    def get_feature_vector(self, prev_y, y, X, t):\n",
        "        return [self.feature_dict[feature_string][(prev_y, y)] for feature_string in extract_text_features_at_position(X, t) if (prev_y, y) in self.feature_dict[feature_string]]\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.label_dict, self.label_array\n",
        "\n",
        "    def calc_inner_products(self, params, X, t):\n",
        "        inner_products = defaultdict(float)\n",
        "        features = chain.from_iterable(self.feature_dict.get(feature_string, {}).items() for feature_string in extract_text_features_at_position(X, t))\n",
        "        for (prev_y, y), feature_id in features:\n",
        "            inner_products[(prev_y, y)] += params[feature_id]\n",
        "        return [((prev_y, y), score) for (prev_y, y), score in inner_products.items()]\n",
        "\n",
        "    def get_empirical_counts(self):\n",
        "        return np.array([self.empirical_counts.get(feature_id, 0) for feature_id in range(self.num_features)])\n",
        "\n",
        "    def get_feature_list(self, X, t):\n",
        "        feature_list_dict = defaultdict(set)\n",
        "        [feature_list_dict[(prev_y, y)].add(feature_id) for feature_string in extract_text_features_at_position(X, t) for (prev_y, y), feature_id in self.feature_dict[feature_string].items()]\n",
        "        return list(feature_list_dict.items())\n",
        "\n",
        "    def serialize_feature_dict(self):\n",
        "        return {feature_string: {'%d_%d' % (prev_y, y): feature_id for (prev_y, y), feature_id in features.items()}\n",
        "                for feature_string, features in self.feature_dict.items()}\n",
        "\n",
        "    def deserialize_feature_dict(self, serialized):\n",
        "        return {feature_string: {(int(prev_y), int(y)): feature_id\n",
        "                for transition_string, feature_id in features.items()\n",
        "                for prev_y, y in [transition_string.split('_')]}\n",
        "                for feature_string, features in serialized.items()}\n",
        "\n",
        "\n",
        "SCALE_THRES = 1e250\n",
        "ITER_NUM = 0\n",
        "SUB_ITER_NUM = 0\n",
        "TOTAL_SUB_ITERS = 0\n",
        "GRAD = None\n",
        "\n",
        "\n",
        "def _gen_trans_prob_tables(params, num_labels, feature_set, X, inference=True):\n",
        "    tables = []\n",
        "    for t, x in enumerate(X):\n",
        "        table = np.zeros((num_labels, num_labels))\n",
        "        pairs = feature_set.calc_inner_products(params, X, t) if inference else x\n",
        "        for pair, score in pairs:\n",
        "            prev_y, y = pair\n",
        "            score = sum(params[fid] for fid in score) if not inference else score\n",
        "            table[prev_y if prev_y != -1 else slice(None), y] += score\n",
        "\n",
        "        table = np.exp(table)\n",
        "        if t == 0:\n",
        "            table[BOS_IDX+1:] = 0\n",
        "        else:\n",
        "            table[:, BOS_IDX] = 0\n",
        "            table[BOS_IDX, :] = 0\n",
        "        tables.append(table)\n",
        "\n",
        "    return tables\n",
        "\n",
        "\n",
        "def _forward_backward(num_labels, time_length, trans_prob_tables):\n",
        "    alpha, beta = np.zeros((time_length, num_labels)), np.zeros((time_length, num_labels))\n",
        "    scaling_dict = {}\n",
        "\n",
        "    alpha[0, :] = trans_prob_tables[0][BOS_IDX, :]\n",
        "    for t in range(1, time_length):\n",
        "        alpha[t] = np.dot(alpha[t-1], trans_prob_tables[t])\n",
        "        if alpha[t].max() > SCALE_THRES:\n",
        "            scaling_dict[t-1] = SCALE_THRES\n",
        "            alpha[t-1] /= SCALE_THRES\n",
        "            alpha[t] = 0\n",
        "            break\n",
        "\n",
        "    beta[-1] = 1.0\n",
        "    for t in range(time_length - 2, -1, -1):\n",
        "        beta[t] = np.dot(beta[t+1], trans_prob_tables[t+1].T)\n",
        "        if t in scaling_dict:\n",
        "            beta[t] /= scaling_dict[t]\n",
        "\n",
        "    Z = alpha[-1].sum()\n",
        "\n",
        "    return alpha, beta, Z, scaling_dict\n",
        "\n",
        "\n",
        "def _log_likelihood(params, *args):\n",
        "    training_data, feature_set, training_feature_data, empirical_counts, label_dict, squared_sigma = args\n",
        "    expected_counts = np.zeros(len(feature_set))\n",
        "    total_logZ = 0\n",
        "\n",
        "    for X_features in training_feature_data:\n",
        "        trans_prob_tables = _gen_trans_prob_tables(params, len(label_dict), feature_set, X_features, inference=False)\n",
        "        alpha, beta, Z, scaling_dict = _forward_backward(len(label_dict), len(X_features), trans_prob_tables)\n",
        "        total_logZ += log(Z) + sum(log(s) for s in scaling_dict.values())\n",
        "\n",
        "        for t, X_feature in enumerate(X_features):\n",
        "            for (prev_y, y), feature_ids in X_feature:\n",
        "                if prev_y == -1:\n",
        "                    prob = (alpha[t, y] * beta[t, y] * scaling_dict.get(t, 1)) / Z\n",
        "                elif t == 0 and prev_y == BOS_IDX:\n",
        "                    prob = (trans_prob_tables[t][BOS_IDX, y] * beta[t, y]) / Z\n",
        "                elif prev_y != BOS_IDX and y != BOS_IDX:\n",
        "                    prob = (alpha[t-1, prev_y] * trans_prob_tables[t][prev_y, y] * beta[t, y]) / Z\n",
        "                else:\n",
        "                    continue\n",
        "                for fid in feature_ids:\n",
        "                    expected_counts[fid] += prob\n",
        "\n",
        "    likelihood = np.dot(empirical_counts, params) - total_logZ - np.sum(params**2) / (2 * squared_sigma)\n",
        "    gradients = empirical_counts - expected_counts - params / squared_sigma\n",
        "    global GRAD\n",
        "    GRAD = gradients\n",
        "\n",
        "    global SUB_ITER_NUM\n",
        "    print(f\"  {ITER_NUM:03d} {f'({SUB_ITER_NUM:02d})' if SUB_ITER_NUM > 0 else '    '}: {-likelihood}\")\n",
        "    SUB_ITER_NUM += 1\n",
        "\n",
        "    return -likelihood\n",
        "\n",
        "\n",
        "def _gradient(params, *args):\n",
        "    return GRAD * -1\n",
        "\n",
        "\n",
        "class LinearChainCRF():\n",
        "    training_data = feature_set = label_dict = label_array = num_labels = params = None\n",
        "    squared_sigma = 10.0\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _read_corpus(self, filename):\n",
        "        return read_corpus(filename)\n",
        "\n",
        "    def _get_training_feature_data(self):\n",
        "        return [[self.feature_set.get_feature_list(X, t) for t in range(len(X))]\n",
        "                for X, _ in self.training_data]\n",
        "\n",
        "    def _estimate_parameters(self):\n",
        "        def _callback(params):\n",
        "            global ITER_NUM, SUB_ITER_NUM, TOTAL_SUB_ITERS\n",
        "            ITER_NUM += 1\n",
        "            TOTAL_SUB_ITERS += SUB_ITER_NUM\n",
        "            SUB_ITER_NUM = 0\n",
        "\n",
        "        # Get training feature data using a method _get_training_feature_data()\n",
        "        train_feat_data = self._get_training_feature_data()\n",
        "\n",
        "        # Perform L-BFGS-B optimization using the fmin_l_bfgs_b function\n",
        "        self.params, log_likelihood, info = fmin_l_bfgs_b(\n",
        "            func=_log_likelihood,  # Objective function to minimize\n",
        "            fprime=_gradient,       # Gradient of the objective function\n",
        "            x0=np.zeros(len(self.feature_set)),  # Initial guess for parameters\n",
        "            args=(\n",
        "                self.training_data,  # Training data\n",
        "                self.feature_set,    # Feature set\n",
        "                train_feat_data,     # Training feature data\n",
        "                self.feature_set.get_empirical_counts(),  # Empirical feature counts\n",
        "                self.label_dict,      # Label dictionary\n",
        "                self.squared_sigma   # Squared sigma value\n",
        "            ),\n",
        "            callback=_callback  # Callback function to be called during optimization\n",
        "        )\n",
        "\n",
        "    def train(self, corpus_filename, model_filename):\n",
        "        self.training_data = self._read_corpus(corpus_filename)\n",
        "        self.feature_set = FeatureSet()\n",
        "        self.feature_set.process_corpus(self.training_data)\n",
        "        self.label_dict, self.label_array = self.feature_set.get_labels()\n",
        "        self.num_labels = len(self.label_array)\n",
        "        self._estimate_parameters()\n",
        "        self.save_model(model_filename)\n",
        "\n",
        "    def test(self, test_corpus_filename):\n",
        "        test_data = self._read_corpus(test_corpus_filename)\n",
        "        total_count, correct_count = 0, 0\n",
        "        for X, Y in test_data:\n",
        "            total_count += len(Y)\n",
        "            correct_count += sum(y == yp for y, yp in zip(Y, self.inference(X)))\n",
        "        print(f'Accuracy: {correct_count/total_count}')\n",
        "\n",
        "    def inference(self, X):\n",
        "        return self.viterbi(X, _gen_trans_prob_tables(self.params, self.num_labels, self.feature_set, X, inference=True))\n",
        "\n",
        "    def viterbi(self, observations, trans_prob_tables):\n",
        "        seq_len = len(observations)\n",
        "        max_prob_table = np.zeros((seq_len, self.num_labels))\n",
        "        backpointer_table = np.zeros((seq_len, self.num_labels), dtype='int64')\n",
        "\n",
        "        max_prob_table[0, :] = trans_prob_tables[0][BOS_IDX, :]\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            for cur_label_id in range(self.num_labels):\n",
        "                probabilities = [max_prob_table[t-1, prev_label_id] *\n",
        "                                trans_prob_tables[t][prev_label_id, cur_label_id]\n",
        "                                for prev_label_id in range(self.num_labels)]\n",
        "\n",
        "                best_prev_label = np.argmax(probabilities)\n",
        "                highest_prob = probabilities[best_prev_label]\n",
        "                max_prob_table[t, cur_label_id] = highest_prob\n",
        "                backpointer_table[t, cur_label_id] = best_prev_label\n",
        "\n",
        "        decoded_sequence = []\n",
        "        last_label = np.argmax(max_prob_table[-1])\n",
        "        decoded_sequence.append(last_label)\n",
        "\n",
        "        for t in range(seq_len - 1, 0, -1):\n",
        "            last_label = backpointer_table[t, last_label]\n",
        "            decoded_sequence.append(last_label)\n",
        "\n",
        "        return [self.label_dict[label_id] for label_id in decoded_sequence[::-1]]\n",
        "\n",
        "    def save_model(self, model_filename):\n",
        "        with open(model_filename, 'w') as f:\n",
        "            json.dump({\n",
        "                \"feature_dict\": self.feature_set.serialize_feature_dict(),\n",
        "                \"num_features\": self.feature_set.num_features,\n",
        "                \"labels\": self.feature_set.label_array,\n",
        "                \"params\": list(self.params)\n",
        "            }, f, ensure_ascii=False, indent=2, separators=(',', ':'))\n",
        "\n",
        "    def load(self, model_filename):\n",
        "        with open(model_filename) as f:\n",
        "            model = json.load(f)\n",
        "\n",
        "        self.feature_set = FeatureSet()\n",
        "        self.feature_set.load(model['feature_dict'], model['num_features'], model['labels'])\n",
        "        self.label_dict, self.label_array = self.feature_set.get_labels()\n",
        "        self.num_labels, self.params = len(self.label_array), np.asarray(model['params'])\n",
        "\n",
        "\n",
        "crf = LinearChainCRF()\n",
        "crf.train('data/train.txt', 'data/model.json')\n",
        "crf.load('data/model.json')\n",
        "crf.test('data/test.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CÃ¢u 2"
      ],
      "metadata": {
        "id": "eg72FGuz-KRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from math import log\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from itertools import chain\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "BOS_TOKEN, BOS_IDX = '<bos>', 0\n",
        "\n",
        "\n",
        "def read_corpus(filename):\n",
        "    with open(filename) as file:\n",
        "        lines = [line.strip().split() for line in file]\n",
        "\n",
        "    data, X, Y = [], [], []\n",
        "    for words in lines:\n",
        "        if not words:\n",
        "            if X: data.append((X, Y))\n",
        "            X, Y = [], []\n",
        "        else:\n",
        "            X.append(words[:-1])\n",
        "            Y.append(words[-1])\n",
        "\n",
        "    if X: data.append((X, Y))\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_text_features_at_position(text_tokens, position):\n",
        "    \"\"\"\n",
        "    Extracts text features at a specified position in a list of text tokens.\n",
        "\n",
        "    Args:\n",
        "    text_tokens (list): A list of text tokens, where each token is represented as a list containing\n",
        "                        the token itself and its part-of-speech tag.\n",
        "    position (int): The position in the list to extract features for.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of extracted features, including unigrams and bigrams based on the specified position.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    For text_tokens:\n",
        "\n",
        "    [['Confidence', 'NN'], ['in', 'IN'], ['the', 'DT'], ['pound', 'NN'], ['is', 'VBZ'], ['widely', 'RB'],\n",
        "     ['expected', 'VBN'], ['to', 'TO'], ['take', 'VB'], ['another', 'DT'], ['sharp', 'JJ'], ['dive', 'NN'],\n",
        "     ['if', 'IN'], ['trade', 'NN'], ['figures', 'NNS'], ['for', 'IN'], ['September', 'NNP'], [',', ','],\n",
        "     ['due', 'JJ'], ['for', 'IN'], ['release', 'NN'], ['tomorrow', 'NN'], [',', ','], ['fail', 'VB'],\n",
        "     ['to', 'TO'], ['show', 'VB'], ['a', 'DT'], ['substantial', 'JJ'], ['improvement', 'NN'], ['from', 'IN'],\n",
        "     ['July', 'NNP'], ['and', 'CC'], ['August', 'NNP'], [\"'s\", 'POS'], ['near-record', 'JJ'], ['deficits', 'NNS'],\n",
        "     ['.', '.']]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Sau khi thÃªm viá»c sá»­ dá»¥ng cÃ¡c Äáº·c trÆ°ng khÃ¡c (Tá»©c lÃ  thÃªm cÃ¡c nhÃ£n tá»« loáº¡i sá»­ dá»¥ng nhÆ° lÃ  má»t Äáº·c trÆ°ng\n",
        "    # vá» máº·t nghÄ©a nghÄ©a). ThÃ¬ mÃ´ hÃ¬nh ÄÃ£ cÃ³ sá»± tÄng trÆ°á»ng vá» hiá»u suáº¥t (Äáº¡t khoáº£ng 89%).\n",
        "    # VÃ­ dá»¥ á» vá» trÃ­ position = 0  thÃ¬:\n",
        "    # features = ['unigram[0]:Confidence','pos_unigram[0]:NN', 'unigram[+1]:in', 'pos_unigram[+1]:IN', 'bigram[0]:Confidence in',\n",
        "    #             'pos_bigram[0]:NN IN',  'unigram[+2]:the', 'pos_unigram[+2]:DT', 'pos_bigram[+1]:IN DT']\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    length = len(text_tokens)\n",
        "\n",
        "    features = list()\n",
        "    features.append('unigram[0]:' + text_tokens[position][0])\n",
        "    features.append('pos_unigram[0]:' + text_tokens[position][1])\n",
        "\n",
        "    if position < length-1:\n",
        "        features.append('unigram[+1]:' + (text_tokens[position+1][0]))\n",
        "        features.append('pos_unigram[+1]:' + text_tokens[position+1][1])\n",
        "        features.append('bigram[0]:{} {}'.format(text_tokens[position][0], text_tokens[position+1][0]))\n",
        "        features.append('pos_bigram[0]:{} {}'.format(text_tokens[position][1], text_tokens[position+1][1]))\n",
        "\n",
        "        if position < length-2:\n",
        "            features.append('unigram[+2]:' + (text_tokens[position+2][0]))\n",
        "            features.append('pos_unigram[+2]:' + (text_tokens[position+2][1]))\n",
        "            features.append('pos_bigram[+1]:{} {}'.format(text_tokens[position+1][1], text_tokens[position+2][1]))\n",
        "\n",
        "    if position > 0:\n",
        "        features.append('unigram[-1]:' + (text_tokens[position-1][0]))\n",
        "        features.append('pos_unigram[-1]:' + (text_tokens[position-1][1]))\n",
        "        features.append('bigram[-1]:{} {}'.format(text_tokens[position-1][0], text_tokens[position][0]))\n",
        "        features.append('pos_bigram[-1]:{} {}'.format(text_tokens[position-1][1], text_tokens[position][1]))\n",
        "\n",
        "        if position > 1:\n",
        "            features.append('unigram[-2]:' + (text_tokens[position-2][0]))\n",
        "            features.append('pos_unigram[-2]:' + (text_tokens[position-2][1]))\n",
        "            features.append('pos_bigram[-2]:{} {}'.format(text_tokens[position-2][1], text_tokens[position-1][1]))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class FeatureSet():\n",
        "    feature_dict, observation_set, empirical_counts = dict(), set(), Counter()\n",
        "    num_features, label_dict, label_array = 0, {BOS_TOKEN: BOS_IDX}, [BOS_TOKEN]\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def process_corpus(self, data):\n",
        "        for X, Y in data:\n",
        "            prev_y = BOS_IDX\n",
        "            for t, char in enumerate(X):\n",
        "                y = self.label_dict.get(Y[t], len(self.label_dict))\n",
        "                if Y[t] not in self.label_dict:\n",
        "                    self.label_dict[Y[t]] = y\n",
        "                    self.label_array.append(Y[t])\n",
        "                self._add(prev_y, y, X, t)\n",
        "                prev_y = y\n",
        "\n",
        "    def load(self, feature_dict, num_features, label_array):\n",
        "        self.num_features = num_features\n",
        "        self.label_array = label_array\n",
        "        self.label_dict = {label: i for label, i in enumerate(label_array)}\n",
        "        self.feature_dict = self.deserialize_feature_dict(feature_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_features\n",
        "\n",
        "    def _add(self, prev_y, y, X, t):\n",
        "        for feature_string in extract_text_features_at_position(X, t):\n",
        "            if feature_string not in self.feature_dict:\n",
        "                self.feature_dict[feature_string] = {}\n",
        "\n",
        "            for pair in [(prev_y, y), (-1, y)]:\n",
        "                if pair not in self.feature_dict[feature_string]:\n",
        "                    self.feature_dict[feature_string][pair] = self.num_features\n",
        "                    self.num_features += 1\n",
        "\n",
        "                feature_id = self.feature_dict[feature_string][pair]\n",
        "                self.empirical_counts[feature_id] += 1\n",
        "\n",
        "    def get_feature_vector(self, prev_y, y, X, t):\n",
        "        return [self.feature_dict[feature_string][(prev_y, y)] for feature_string in extract_text_features_at_position(X, t) if (prev_y, y) in self.feature_dict[feature_string]]\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.label_dict, self.label_array\n",
        "\n",
        "    def calc_inner_products(self, params, X, t):\n",
        "        inner_products = defaultdict(float)\n",
        "        features = chain.from_iterable(self.feature_dict.get(feature_string, {}).items() for feature_string in extract_text_features_at_position(X, t))\n",
        "        for (prev_y, y), feature_id in features:\n",
        "            inner_products[(prev_y, y)] += params[feature_id]\n",
        "        return [((prev_y, y), score) for (prev_y, y), score in inner_products.items()]\n",
        "\n",
        "    def get_empirical_counts(self):\n",
        "        return np.array([self.empirical_counts.get(feature_id, 0) for feature_id in range(self.num_features)])\n",
        "\n",
        "    def get_feature_list(self, X, t):\n",
        "        feature_list_dict = defaultdict(set)\n",
        "        [feature_list_dict[(prev_y, y)].add(feature_id) for feature_string in extract_text_features_at_position(X, t) for (prev_y, y), feature_id in self.feature_dict[feature_string].items()]\n",
        "        return list(feature_list_dict.items())\n",
        "\n",
        "    def serialize_feature_dict(self):\n",
        "        return {feature_string: {'%d_%d' % (prev_y, y): feature_id for (prev_y, y), feature_id in features.items()}\n",
        "                for feature_string, features in self.feature_dict.items()}\n",
        "\n",
        "    def deserialize_feature_dict(self, serialized):\n",
        "        return {feature_string: {(int(prev_y), int(y)): feature_id\n",
        "                for transition_string, feature_id in features.items()\n",
        "                for prev_y, y in [transition_string.split('_')]}\n",
        "                for feature_string, features in serialized.items()}\n",
        "\n",
        "\n",
        "SCALE_THRES = 1e250\n",
        "ITER_NUM = 0\n",
        "SUB_ITER_NUM = 0\n",
        "TOTAL_SUB_ITERS = 0\n",
        "GRAD = None\n",
        "\n",
        "\n",
        "def _gen_trans_prob_tables(params, num_labels, feature_set, X, inference=True):\n",
        "    tables = []\n",
        "    for t, x in enumerate(X):\n",
        "        table = np.zeros((num_labels, num_labels))\n",
        "        pairs = feature_set.calc_inner_products(params, X, t) if inference else x\n",
        "        for pair, score in pairs:\n",
        "            prev_y, y = pair\n",
        "            score = sum(params[fid] for fid in score) if not inference else score\n",
        "            table[prev_y if prev_y != -1 else slice(None), y] += score\n",
        "\n",
        "        table = np.exp(table)\n",
        "        if t == 0:\n",
        "            table[BOS_IDX+1:] = 0\n",
        "        else:\n",
        "            table[:, BOS_IDX] = 0\n",
        "            table[BOS_IDX, :] = 0\n",
        "        tables.append(table)\n",
        "\n",
        "    return tables\n",
        "\n",
        "\n",
        "def _forward_backward(num_labels, time_length, trans_prob_tables):\n",
        "    alpha, beta = np.zeros((time_length, num_labels)), np.zeros((time_length, num_labels))\n",
        "    scaling_dict = {}\n",
        "\n",
        "    alpha[0, :] = trans_prob_tables[0][BOS_IDX, :]\n",
        "    for t in range(1, time_length):\n",
        "        alpha[t] = np.dot(alpha[t-1], trans_prob_tables[t])\n",
        "        if alpha[t].max() > SCALE_THRES:\n",
        "            scaling_dict[t-1] = SCALE_THRES\n",
        "            alpha[t-1] /= SCALE_THRES\n",
        "            alpha[t] = 0\n",
        "            break\n",
        "\n",
        "    beta[-1] = 1.0\n",
        "    for t in range(time_length - 2, -1, -1):\n",
        "        beta[t] = np.dot(beta[t+1], trans_prob_tables[t+1].T)\n",
        "        if t in scaling_dict:\n",
        "            beta[t] /= scaling_dict[t]\n",
        "\n",
        "    Z = alpha[-1].sum()\n",
        "\n",
        "    return alpha, beta, Z, scaling_dict\n",
        "\n",
        "\n",
        "def _log_likelihood(params, *args):\n",
        "    training_data, feature_set, training_feature_data, empirical_counts, label_dict, squared_sigma = args\n",
        "    expected_counts = np.zeros(len(feature_set))\n",
        "    total_logZ = 0\n",
        "\n",
        "    for X_features in training_feature_data:\n",
        "        trans_prob_tables = _gen_trans_prob_tables(params, len(label_dict), feature_set, X_features, inference=False)\n",
        "        alpha, beta, Z, scaling_dict = _forward_backward(len(label_dict), len(X_features), trans_prob_tables)\n",
        "        total_logZ += log(Z) + sum(log(s) for s in scaling_dict.values())\n",
        "\n",
        "        for t, X_feature in enumerate(X_features):\n",
        "            for (prev_y, y), feature_ids in X_feature:\n",
        "                if prev_y == -1:\n",
        "                    prob = (alpha[t, y] * beta[t, y] * scaling_dict.get(t, 1)) / Z\n",
        "                elif t == 0 and prev_y == BOS_IDX:\n",
        "                    prob = (trans_prob_tables[t][BOS_IDX, y] * beta[t, y]) / Z\n",
        "                elif prev_y != BOS_IDX and y != BOS_IDX:\n",
        "                    prob = (alpha[t-1, prev_y] * trans_prob_tables[t][prev_y, y] * beta[t, y]) / Z\n",
        "                else:\n",
        "                    continue\n",
        "                for fid in feature_ids:\n",
        "                    expected_counts[fid] += prob\n",
        "\n",
        "    likelihood = np.dot(empirical_counts, params) - total_logZ - np.sum(params**2) / (2 * squared_sigma)\n",
        "    gradients = empirical_counts - expected_counts - params / squared_sigma\n",
        "    global GRAD\n",
        "    GRAD = gradients\n",
        "\n",
        "    global SUB_ITER_NUM\n",
        "    print(f\"  {ITER_NUM:03d} {f'({SUB_ITER_NUM:02d})' if SUB_ITER_NUM > 0 else '    '}: {-likelihood}\")\n",
        "    SUB_ITER_NUM += 1\n",
        "\n",
        "    return -likelihood\n",
        "\n",
        "\n",
        "def _gradient(params, *args):\n",
        "    return GRAD * -1\n",
        "\n",
        "\n",
        "class LinearChainCRF():\n",
        "    training_data = feature_set = label_dict = label_array = num_labels = params = None\n",
        "    squared_sigma = 10.0\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _read_corpus(self, filename):\n",
        "        return read_corpus(filename)\n",
        "\n",
        "    def _get_training_feature_data(self):\n",
        "        return [[self.feature_set.get_feature_list(X, t) for t in range(len(X))]\n",
        "                for X, _ in self.training_data]\n",
        "\n",
        "    def _estimate_parameters(self):\n",
        "        def _callback(params):\n",
        "            global ITER_NUM, SUB_ITER_NUM, TOTAL_SUB_ITERS\n",
        "            ITER_NUM += 1\n",
        "            TOTAL_SUB_ITERS += SUB_ITER_NUM\n",
        "            SUB_ITER_NUM = 0\n",
        "\n",
        "        # Get training feature data using a method _get_training_feature_data()\n",
        "        train_feat_data = self._get_training_feature_data()\n",
        "\n",
        "        # Perform L-BFGS-B optimization using the fmin_l_bfgs_b function\n",
        "        self.params, log_likelihood, info = fmin_l_bfgs_b(\n",
        "            func=_log_likelihood,  # Objective function to minimize\n",
        "            fprime=_gradient,       # Gradient of the objective function\n",
        "            x0=np.zeros(len(self.feature_set)),  # Initial guess for parameters\n",
        "            args=(\n",
        "                self.training_data,  # Training data\n",
        "                self.feature_set,    # Feature set\n",
        "                train_feat_data,     # Training feature data\n",
        "                self.feature_set.get_empirical_counts(),  # Empirical feature counts\n",
        "                self.label_dict,      # Label dictionary\n",
        "                self.squared_sigma   # Squared sigma value\n",
        "            ),\n",
        "            callback=_callback  # Callback function to be called during optimization\n",
        "        )\n",
        "\n",
        "    def train(self, corpus_filename, model_filename):\n",
        "        self.training_data = self._read_corpus(corpus_filename)\n",
        "        self.feature_set = FeatureSet()\n",
        "        self.feature_set.process_corpus(self.training_data)\n",
        "        self.label_dict, self.label_array = self.feature_set.get_labels()\n",
        "        self.num_labels = len(self.label_array)\n",
        "        self._estimate_parameters()\n",
        "        self.save_model(model_filename)\n",
        "\n",
        "    def test(self, test_corpus_filename):\n",
        "        test_data = self._read_corpus(test_corpus_filename)\n",
        "        total_count, correct_count = 0, 0\n",
        "        for X, Y in test_data:\n",
        "            total_count += len(Y)\n",
        "            correct_count += sum(y == yp for y, yp in zip(Y, self.inference(X)))\n",
        "        print(f'Accuracy: {correct_count/total_count}')\n",
        "\n",
        "    def inference(self, X):\n",
        "        return self.viterbi(X, _gen_trans_prob_tables(self.params, self.num_labels, self.feature_set, X, inference=True))\n",
        "\n",
        "    def viterbi(self, observations, trans_prob_tables):\n",
        "        seq_len = len(observations)\n",
        "        max_prob_table = np.zeros((seq_len, self.num_labels))\n",
        "        backpointer_table = np.zeros((seq_len, self.num_labels), dtype='int64')\n",
        "\n",
        "        max_prob_table[0, :] = trans_prob_tables[0][BOS_IDX, :]\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            for cur_label_id in range(self.num_labels):\n",
        "                probabilities = [max_prob_table[t-1, prev_label_id] *\n",
        "                                trans_prob_tables[t][prev_label_id, cur_label_id]\n",
        "                                for prev_label_id in range(self.num_labels)]\n",
        "\n",
        "                best_prev_label = np.argmax(probabilities)\n",
        "                highest_prob = probabilities[best_prev_label]\n",
        "                max_prob_table[t, cur_label_id] = highest_prob\n",
        "                backpointer_table[t, cur_label_id] = best_prev_label\n",
        "\n",
        "        decoded_sequence = []\n",
        "        last_label = np.argmax(max_prob_table[-1])\n",
        "        decoded_sequence.append(last_label)\n",
        "\n",
        "        for t in range(seq_len - 1, 0, -1):\n",
        "            last_label = backpointer_table[t, last_label]\n",
        "            decoded_sequence.append(last_label)\n",
        "\n",
        "        return [self.label_dict[label_id] for label_id in decoded_sequence[::-1]]\n",
        "\n",
        "    def save_model(self, model_filename):\n",
        "        with open(model_filename, 'w') as f:\n",
        "            json.dump({\n",
        "                \"feature_dict\": self.feature_set.serialize_feature_dict(),\n",
        "                \"num_features\": self.feature_set.num_features,\n",
        "                \"labels\": self.feature_set.label_array,\n",
        "                \"params\": list(self.params)\n",
        "            }, f, ensure_ascii=False, indent=2, separators=(',', ':'))\n",
        "\n",
        "    def load(self, model_filename):\n",
        "        with open(model_filename) as f:\n",
        "            model = json.load(f)\n",
        "\n",
        "        self.feature_set = FeatureSet()\n",
        "        self.feature_set.load(model['feature_dict'], model['num_features'], model['labels'])\n",
        "        self.label_dict, self.label_array = self.feature_set.get_labels()\n",
        "        self.num_labels, self.params = len(self.label_array), np.asarray(model['params'])\n",
        "\n",
        "\n",
        "crf = LinearChainCRF()\n",
        "crf.train('data/train.txt', 'data/model.json')\n",
        "crf.load('data/model.json')\n",
        "crf.test('data/test.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnBIUSky8n39",
        "outputId": "3a2595fb-db53-4e5c-c1d6-051c421f189d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  000     : 5003.65269695053\n",
            "  000 (01): 4083.068117640481\n",
            "  000 (02): 1977.450564661489\n",
            "  001     : 1150.3799168504424\n",
            "  002     : 552.0368631063133\n",
            "  003     : 225.25516555802585\n",
            "  004     : 93.66380689689714\n",
            "  005     : 51.21819407559596\n",
            "  006     : 45.66223501989544\n",
            "  007     : 40.70651233120603\n",
            "  008     : 37.7004188405528\n",
            "  009     : 36.14815403807255\n",
            "  010     : 35.01745990905319\n",
            "  011     : 34.3559615315225\n",
            "  012     : 33.537198055897086\n",
            "  013     : 33.041188641441\n",
            "  014     : 32.74473232070475\n",
            "  015     : 32.5276782997862\n",
            "  016     : 32.39373286627248\n",
            "  017     : 32.32056243753058\n",
            "  018     : 32.26829357768802\n",
            "  019     : 32.23335244907998\n",
            "  020     : 32.19914218681741\n",
            "  021     : 32.176077992439794\n",
            "  022     : 32.16269675814492\n",
            "  023     : 32.15774916825298\n",
            "  024     : 32.15421816788401\n",
            "  025     : 32.15079756134682\n",
            "  026     : 32.14968209709789\n",
            "  027     : 32.148141727289044\n",
            "  028     : 32.14778749426404\n",
            "  029     : 32.14727586585118\n",
            "  030     : 32.147750351698676\n",
            "  030 (01): 32.14717642268366\n",
            "  031     : 32.14701588961452\n",
            "  032     : 32.146907586719806\n",
            "  033     : 32.146839373008504\n",
            "  034     : 32.146812504368214\n",
            "  035     : 32.14679247577598\n",
            "  036     : 32.14677856042299\n",
            "  037     : 32.14677027499856\n",
            "  038     : 32.14676564341334\n",
            "  039     : 32.14676110564912\n",
            "  040     : 32.14675988777364\n",
            "  041     : 32.14675836634132\n",
            "  042     : 32.14675769107964\n",
            "  043     : 32.146756631987095\n",
            "  044     : 32.14675644497551\n",
            "  045     : 32.1467561420901\n",
            "  046     : 32.1467559973639\n",
            "  047     : 32.14675585457973\n",
            "  048     : 32.14675582088566\n",
            "Accuracy: 0.899958272480701\n"
          ]
        }
      ]
    }
  ]
}